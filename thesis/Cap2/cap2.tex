\section{Experiment Setup}
\subsection{Motivation}

During Automated Program Repair (APR) activities, traditional methods (based on heuristics or templates) often generate syntactically correct changes, but they fall short in the semantic and code style aspects (insert ref).

In this context, Large Language Models (LLMs) are capable of capturing code patterns and semantic context with high precision.
On the other hand, LLMs exhibit significant sensitivity to how their instructions (prompts) are phrased. Different instructions can alter the fidelity to the original style, the clarity of the patch, and the size of the modification, especially if little context is provided during the correction process (insert ref).

\section{Dataset and datapreparation}

\subsection{Pytracebugs}
The experiment uses the Pytracebugs dataset \cite{Pytracebugs}, which contains Python source codes from GitHub repositories at the granularity of function snippets and their corresponding Traceback errors; see the example below.


\begin{figure}[h!]
    \centering
    \caption{An example of a single data point from the Pytracebugs dataset.}
    \label{fig:dataset-example}

    % (a) The Buggy Code
    \begin{lstlisting}[
        language=Python,
        frame=single,
        basicstyle=\ttfamily\small,
        caption={a) Buggy Code Snippet (\texttt{before\_merge})},
        belowcaptionskip=-0.5em % Adjust spacing
        ]
def noise(self, value):
  self.noise_covar.initialize(value)
    \end{lstlisting}

    % (b) The Traceback Error
    \begin{lstlisting}[
        frame=single,
        basicstyle=\ttfamily\small,
        caption={b) Traceback Error (\texttt{full\_traceback})},
        belowcaptionskip=-0.5em % Adjust spacing
        ]
import gpytorch
gl = gpytorch.likelihoods.GaussianLikelihood()
gl.initialize(noise=1)
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File "".../gpytorch/gpytorch/module.py"", line 89, in initialize
setattr(self, name, val)
File ""../lib/python3.6/site-packages/torch/nn/modules/module.py"", line 579, in __setattr__
object.__setattr__(self, name, value)
File "".../gpytorch/gpytorch/likelihoods/gaussian_likelihood.py"", line 63, in noise
self.noise_covar.initialize(value)
TypeError: initialize() takes 1 positional argument but 2 were given
    \end{lstlisting}

    % (c) The Fixed Code
    \begin{lstlisting}[
        language=Python,
        frame=single,
        basicstyle=\ttfamily\small,
        caption={c) Fixed Code Snippet (\texttt{after\_merge})}
        ]
def noise(self, value):
  self.noise_covar.initialize(noise=value)
    \end{lstlisting}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Cap2/dataset-traceback-type.png}
    \caption{Distribution of Error Types in the dataset - Less than 1\% relevant are consolidated into 'others' category.}
    \label{fig:dataset-traceback-types}
\end{figure}

Most frequent traceback errors can be found in Figure \ref{fig:dataset-traceback-types}, with the most frequent ones being Atribute Error and TypeError with $16.84\%$ and $16\%$ of the values respectivelly. Also, there is a significant amount of erros that appear less than $1\%$ of the time, and were aggregated into the 'Others' Category.

%TODO: Adicionar exemplos de trabalhos publicados que utilizam como referencia o dataset proposto.
Some published works that use this dataset aim to study problems such as Vulnerability Detection \cite{zhao2024coding} and fault localization \cite{kulkarni2024graph}.

\subsection{LLM Models}
The model selection for this experiment was guided by three criteria: (1) representation of different architectural approaches, (2) availability through OpenRouter API, and (3) cost-effectiveness for large-scale experimentation. Four models were selected to provide a balanced comparison across these dimensions.

Two open-source models specialized in code generation were included: Qwen 2.5 Coder 32B (instruct) \cite{hui2024qwen25codertechnicalreport} and Codestral 2501 \cite{Codestral_202501}. These models represent the current state-of-the-art in open-source code generation, with Qwen 2.5 Coder 32B achieving competitive performance on coding benchmarks while maintaining significantly lower computational requirements compared to larger models.

Two closed-source models were selected for comparison: Claude 3.5 Sonnet and GPT-4o. These models represent the current frontier of general-purpose language models and provide a baseline for evaluating whether specialized code models offer advantages over general-purpose architectures in automated program repair tasks.

The selection of exactly four models balances statistical power requirements with computational feasibility. While larger model sets could provide more comprehensive comparisons, the chosen sample size allows for robust statistical analysis while maintaining manageable experimental costs.

% TODO: adicionar referencias que confirmem que os modelos estão dentre os modelos de estado da arte no período. Benchmarks

%TODO: encontrar referências que comprovem que modelos especialistas tem vantagens específicas.

%TODO: Pesquisar referencias de statistical power LLM experiments. para justificar por que 4 modelos são suficientes.

%TODO: Argumentar por que outros modelos não foram escolhidos.

% For the first experiment, four models were chosen. Two of these are open-source models specialized in code generation, considered state-of-the-art in this category: Qwen 2.5 coder 32b (instruct) \cite{hui2024qwen25codertechnicalreport} and Codestral 2501 \cite{Codestral_202501}. Additionally, two state-of-the-art, commonly used closed-source models of a relatively similar size were selected: Claude $3.5$ Sonnet and GPT $4o$ Model.

Although these models are all similarly competitive in structured benchmarks, they differ considerably in terms of usage cost. For reference, the token cost for each selected model is shown in Table \ref{tab:model-tokens}.
\begin{table}[h!]
\centering
\caption{Token usage price per model.}
\label{tab:model-tokens}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model} & \textbf{Input Tokens (1M)} & \textbf{Output Tokens (1M)} \\ \hline
Claude Sonnet 3.5 & \$3.0 & \$15.0 \\ \hline
Gpt 4o & \$2.5 & \$10 \\ \hline
Codestral 2501 & \$0.3 & \$0.9 \\ \hline
Qwen 2.5 Coder 32B Instruct & \$0.05 & \$0.20 \\ \hline
\end{tabular}
\end{table}
% todo addicionar gemini flas
%TODO melhorar a argumentação desses modelos
% Falar do tamanho dos modelos e da comparabilidade deles. Exibir ranking. Existem muitos, portanto qualquer escolha tem uma arbitrariedade. Não vamos todos no hugging faces.

% TODO trazer artigos que divulgam esses modelos.

\section{Experimental Design}
\subsection{Prompt variations}

To test the model's sensitivity to the prompt's instructions, all prompts were formatted 
using a consistent template, shown in Listing \ref{lst:prompt-template}. This structure ensures 
that each model receives the context—buggy code and traceback error—in a clear, delimited 
format, and is explicitly asked to return only the corrected code.

% The 'figure' environment helps manage the placement and captioning of the listing.
\begin{figure}[h!]
% This is the crucial line that starts the code block.
% The formatting options are placed in the square brackets.
\begin{lstlisting}[
    caption={The general template used for all prompts.},
    label={lst:prompt-template},
    basicstyle=\ttfamily\small,
    frame=single,
    breaklines=true
]
{{ instruction_prompt }}

### BUGGY CODE:
{{ buggy_code }}

### ERROR:
{{ traceback_error }}

### RETURN ONLY THE CORRECTED CODE BELOW:

IMPORTANT: Return ONLY the corrected/requested code. Do not include any explanations, comments about the changes, or other text. Just return the pure code.

\end{lstlisting} % This line correctly ends the code block.
\end{figure}
% It clearly isolates the variable you are testing.
\begin{table}[h!]
\centering
\caption{Instruction variations for the \texttt{instruction\_prompt} variable.}
\label{tab:prompt-instructions}
\begin{tabular}{|l|p{0.55\textwidth}|p{0.15\textwidth}|}
\hline
\textbf{Prompt ID} & \textbf{Instruction Text} & \textbf{System Prompt} \\ \hline
P1 (Baseline) & You are a helpful assistant that corrects the code based on the traceback error. & False. \\\hline
P2 (Style-aware) & You are a helpful assistant that corrects the code based on the traceback error. You must respect the original code structure and the original code style. & False. \\ \hline
P3 (System Prompt) & You are a helpful assistant that corrects the code based on the traceback error. & True. \\ \hline
\end{tabular}
\end{table}
The core of the experiment lies in the variation of the \texttt{instruction\_prompt} placeholder and the addition of system prompt. 
We designed three distinct instructions, which we will refer to as P1 (Baseline) and P2 (Style-aware) and P3 (System Prompt), 
to evaluate the impact of a more detailed directive. The specific text for each instruction 
is detailed in Table \ref{tab:prompt-instructions}.

\subsubsection{Justification of Prompt Variations}
The selection of these three specific prompt variations was driven by theoretical considerations in prompt engineering and their relevance to automated program repair tasks. Each variation was designed to test a distinct hypothesis about how LLMs process and respond to different types of instructions.

\textbf{P1 (Baseline):} This prompt serves as the experimental control, providing minimal instruction to establish a baseline performance level. The simplicity of this prompt allows us to measure the inherent capability of each model without additional constraints, serving as a reference point for evaluating the effectiveness of more specific instructions. This approach follows the principle of establishing a null hypothesis in experimental design, where we can assess whether additional prompt complexity actually improves performance.

\textbf{P2 (Style-aware):} The style-aware prompt introduces explicit constraints about preserving code structure and style, addressing a critical concern in automated program repair. Previous research has shown that LLMs can generate syntactically correct but stylistically inconsistent code \cite{wang2025functionalcorrectnessinvestigatingcoding}, which may reduce code maintainability and increase cognitive load for developers. By explicitly instructing the model to respect original code structure and style, we test whether such constraints lead to more maintainable and contextually appropriate patches. This variation is particularly relevant for production environments where code consistency is paramount.

\textbf{P3 (System Prompt):} This variation tests the hypothesis that system-level instructions, which have higher precedence in the API message hierarchy, can provide more consistent and reliable behavior than instruction-level prompts. System prompts are designed to establish global behavioral patterns and are processed differently by the model's architecture. By moving the instruction to the system level, we investigate whether this architectural difference results in more deterministic and consistent code generation, which is crucial for automated repair systems that require reliability and reproducibility.

The systematic variation between these three conditions allows us to isolate the effects of instruction specificity (P1 vs. P2) and instruction placement (P2 vs. P3), providing insights into both the content and delivery mechanisms that influence LLM performance in code repair tasks.

\subsubsection{Prompt Roles in API requests}
\begin{itemize}
    \item \textbf{System prompt:} It is an initialization message that sets global behavior for the model across a conversation or a request. It encodes policies, personas, output formatting, and safety constraints. In most APIs, it has a higher precedence than user instructions. The typical contents usually are guardrails, style registers, and output contracts (e.g., ``always return strict python code'').
    \item \textbf{Instruction prompt:} describes a specific task to perform. it is scoped to the current step/turn and it is subordinate to the system message when they conflict. Typical contents usually are task descriptions, inputs, and desired outputs for the tasks.
\end{itemize}

At inference, the API concatenates messages (System -> Developer -> user->assistant history) into a single token sequence. "Priority" is learned behavior, not a hard rule.

\subsection{Experiment tracking}
The experimental design employed a systematic sampling approach to ensure statistical validity while maintaining computational feasibility. A random sample of 300 buggy code instances was selected from the Pytracebugs dataset for each model-prompt combination, resulting in a total of 4 × 300 × 3 = 3600 experimental trials.

To estimate the statistical power for our sample size, we approximated the paired comparison using Cohen's $d$ and a normal distribution. For $n=300$ paired instances per condition, a family-wise significance level $\alpha_\text{family}=0.01$ with $K=4$ comparisons yields an individual threshold
\[
\alpha_\text{indiv} = \frac{\alpha_\text{family}}{K} = 0.0025,
\]
corresponding to
\[
z_{1-\alpha_\text{indiv}/2} \approx 3.09.
\]
Assuming a small-to-medium effect size of $d \approx 0.3$, the non-central parameter is
\[
\text{ncp} = d \cdot \sqrt{n} \approx 5.196.
\]
The resulting approximate power is
\[
\text{Power} \approx 1 - \Phi(z_{1-\alpha_\text{indiv}/2} - \text{ncp}) \approx 0.982,
\]
indicating that the chosen sample size is sufficient to detect meaningful effects while keeping computational cost reasonable.


% TODO: descrever escolha randomica de experimento e tamanho de amostra que resulte em Significancia estatística necessária para tirarmos conclusões a respeito dos resultados.

To ensure statistical rigor in our evaluation, we conducted a power analysis to verify that our sample size of 300 paired instances per condition was adequate. We used a family-wise significance threshold of $\alpha = 0.01$ for confirmatory comparisons, applying corrections for multiple testing across models. With this setup, 300 samples provide high power (approximately 0.98) to detect effects of small-to-medium size (Cohen's $d \approx 0.3$), which matches the expected effect sizes in this task. Detecting very small effects ($d \approx 0.2$) would require larger samples, but our chosen size balances computational cost and statistical validity, ensuring that meaningful differences can be detected with strong confidence.

For each response, the following evaluation metrics were computed: AST similarity, AST-normalized score, CodeBLEU, N-gram overlap, weighted N-gram match, syntax correctness, and dataflow preservation. Each metric was evaluated against both the original buggy code and the ground truth correction to provide comprehensive performance assessment.

\section{Evaluation method}

\section{Evaluation Method}
\label{sec:evaluation-method}

The evaluation framework employs multiple complementary metrics to assess the quality of generated code repairs across different dimensions: structural similarity, semantic preservation, and syntactic correctness. This multi-faceted approach ensures comprehensive assessment of LLM performance in automated program repair tasks.

\subsection{Structural Similarity Metrics}

\subsubsection{AST Score}
The Abstract Syntax Tree (AST) score measures the structural similarity between the generated repair and the ground truth correction by comparing their parse tree representations. This metric is particularly relevant for automated program repair as it captures the fundamental program structure independently of formatting, variable names, or comment variations.

The AST score is computed by:
\begin{enumerate}
    \item Parsing both the generated code and ground truth into their respective AST representations
    \item Computing the tree edit distance using the Zhang-Shasha algorithm \cite{zhang1989simple}
    \item Normalizing the distance by the maximum tree size to obtain a similarity score in the range [0,1]
\end{enumerate}

A score of 1.0 indicates perfect structural similarity, while 0.0 represents completely different program structures. This metric is sensitive to control flow changes, function structure modifications, and overall program architecture, making it essential for evaluating whether the LLM has preserved the intended program logic.

\subsubsection{AST Normalized Score}
The AST normalized score addresses a fundamental limitation of the raw AST score by normalizing identifiers and constants to focus purely on structural similarity. This metric is particularly valuable in automated program repair as it distinguishes between structural changes and mere naming variations, which are often irrelevant to the actual repair quality.
% TODO: Confirm this
The normalization process involves:
\begin{enumerate}
    \item Parsing both the generated code and ground truth into AST representations
    \item Applying a normalization transformation that:
        \begin{itemize}
            \item Replaces variable names with normalized identifiers (\texttt{\_var\_1}, \texttt{\_var\_2}, etc.)
            \item Replaces function names with normalized identifiers (\texttt{\_func\_1}, \texttt{\_func\_2}, etc.)
            \item Replaces class names with normalized identifiers (\texttt{\_class\_1}, \texttt{\_class\_2}, etc.)
            \item Replaces literal constants with a placeholder (\texttt{\_const\_})
        \end{itemize}
    \item Computing similarity between the normalized AST representations using sequence matching
\end{enumerate}

This approach is crucial for automated program repair evaluation because it focuses on the essential structural logic rather than superficial naming differences. For example, a repair that changes variable names from \texttt{user\_input} to \texttt{input\_data} would receive a perfect normalized AST score, while maintaining the same structural integrity. This metric is particularly valuable for identifying whether LLMs are generating repairs that preserve the intended program logic and control flow, regardless of identifier choices.

To illustrate the difference between AST and AST normalized representations, consider the following example of a Python function:

\begin{figure}[h!]
\centering
\begin{minipage}{0.46\textwidth}
\centering
\textbf{Original AST}
\begin{lstlisting}[basicstyle=\ttfamily\tiny, frame=single, breaklines=true]
Module(
  body=[
  FunctionDef(
    name='check_status',
    args=arguments(
      posonlyargs=[],
      args=[
        arg(arg='monster'),
        arg(arg='status_name')],
      kwonlyargs=[],
      kw_defaults=[],
      defaults=[]),
    body=[
      Return(
        value=Call(
          func=Name(id='any', ctx=Load()),
          args=[
            GeneratorExp(
              elt=Name(id='t', ctx=Load()),
              generators=[
                comprehension(
                  target=Name(id='t', ctx=Store()),
                  iter=Attribute(
                    value=Name(id='monster', ctx=Load()),
                    attr='status',
                    ctx=Load()),
                  ifs=[
                    Compare(
                      left=Attribute(
                        value=Name(id='t', ctx=Load()),
                        attr='name',
                        ctx=Load()),
                      ops=[
                        Eq()],
                      comparators=[
                        Name(id='status_name', ctx=Load())])],
                  is_async=0)])],
          keywords=[]))],
    decorator_list=[])],
  type_ignores=[])
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{0.50\textwidth}
\centering
\textbf{AST Normalized}
\begin{lstlisting}[basicstyle=\ttfamily\tiny, frame=single, breaklines=true]
Module(
  body=[
  FunctionDef(
    name='_func_1',
    args=arguments(
      posonlyargs=[],
      args=[
        arg(arg='monster'),
        arg(arg='status_name')],
      kwonlyargs=[],
      kw_defaults=[],
      defaults=[]),
    body=[
      Return(
        value=Call(
          func=Name(id='_var_1', ctx=Load()),
          args=[
            GeneratorExp(
              elt=Name(id='_var_2', ctx=Load()),
              generators=[
                comprehension(
                  target=Name(id='_var_2', ctx=Store()),
                  iter=Attribute(
                    value=Name(id='_var_4', ctx=Load()),
                    attr='status',
                    ctx=Load()),
                  ifs=[
                    Compare(
                      left=Attribute(
                        value=Name(id='_var_2', ctx=Load()),
                        attr='name',
                        ctx=Load()),
                      ops=[
                        Eq()],
                      comparators=[
                        Name(id='_var_6', ctx=Load())])],
                  is_async=0)])],
          keywords=[]))],
    decorator_list=[])],
  type_ignores=[])
\end{lstlisting}
\end{minipage}
\caption{Side-by-side comparison of AST and AST normalized representations for the same Python function. The normalized version replaces specific identifiers with generic placeholders (\texttt{\_func\_1}, \texttt{\_var\_1}, etc.) while preserving the structural logic.}
\label{fig:ast-comparison}
\end{figure}

As shown in Figure \ref{fig:ast-comparison}, the AST normalized representation replaces specific identifiers like \texttt{check\_status}, \texttt{any}, \texttt{monster}, \texttt{status\_name}, and \texttt{t} with generic placeholders (\texttt{\_func\_1}, \texttt{\_var\_1}, \texttt{\_var\_4}, \texttt{\_var\_6}, and \texttt{\_var\_2} respectively). This normalization allows the metric to focus purely on structural similarity, ignoring naming variations that are irrelevant to the actual repair quality.

The AST normalized score provides a more robust assessment of repair quality by isolating structural changes from naming conventions, making it essential for evaluating whether the core program architecture and logic flow have been preserved in the generated repair.

\section{Implementation Details}
\subsection{control variables}
\label{sec:implementation-details}

All API calls to the proprietary models (GPT-4o and Claude 3.5 Sonnet) and the open-source models (Qwen 2.5 and Codestral) were managed through the \textbf{OpenRouter} API aggregation service. This approach was chosen to ensure a consistent and reproducible experimental setup across all models from a single interface.

The specific model identifiers used on the platform are listed below. To ensure that the results were as deterministic as possible and to facilitate a fair comparison, the generation parameters were kept constant for all API calls: a \textbf{temperature of 0} and a \textbf{top\_p of 1.0} were used. All experiments were conducted in \textbf{July 2025}.
\subsection{retry logic and monitoring}

%todo: explicar melhor como gerenciamos os requests da api.
todo
\begin{itemize}
    \item \texttt{openai/gpt-4o}
    \item \texttt{anthropic/claude-3.5-sonnet}
    \item \texttt{qwen/qwen-2.5-coder-32b-instruct} 
    \item \texttt{mistralai/codestral-2501}
\end{itemize}

\section{Experimental Results Summary}
Table \ref{tab:comprehensive-results} presents the comprehensive results of our experiment, showing the performance of all four models across the three prompt conditions for each evaluation metric. The results are formatted as mean ± standard deviation to provide both central tendency and variability measures.

\begin{table}[h!]
\centering
\caption{Comprehensive metrics comparison across all models and prompt conditions (mean ± std).}
\label{tab:comprehensive-results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|c|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Prompt} & \textbf{AST Score} & \textbf{Text Score} & \textbf{AST Norm.} & \textbf{CodeBLEU} & \textbf{N-gram} & \textbf{W. N-gram} & \textbf{Syntax} & \textbf{Dataflow} \\
\hline
\multirow{3}{*}{Claude 3.5 Sonnet} & Baseline & $0.60 \pm 0.34$ & $0.72 \pm 0.23$ & $0.61 \pm 0.34$ & $0.73 \pm 0.17$ & $0.64 \pm 0.27$ & $0.71 \pm 0.24$ & $0.76 \pm 0.19$ & $0.71 \pm 0.26$ \\
\cline{2-10}
& Style-aware & $0.61 \pm 0.34$ & $0.72 \pm 0.23$ & $0.62 \pm 0.33$ & $0.73 \pm 0.17$ & $0.64 \pm 0.26$ & $0.72 \pm 0.24$ & $0.77 \pm 0.19$ & $0.71 \pm 0.26$ \\
\cline{2-10}
& System & $0.55 \pm 0.37$ & $0.72 \pm 0.22$ & $0.57 \pm 0.37$ & $0.73 \pm 0.17$ & $0.65 \pm 0.26$ & $0.71 \pm 0.24$ & $0.76 \pm 0.19$ & $0.72 \pm 0.26$ \\
\hline
\multirow{3}{*}{Codestral 2501} & Baseline & $0.76 \pm 0.28$ & $0.79 \pm 0.22$ & $0.77 \pm 0.27$ & $0.78 \pm 0.19$ & $0.73 \pm 0.26$ & $0.77 \pm 0.24$ & $0.81 \pm 0.19$ & $0.74 \pm 0.27$ \\
\cline{2-10}
& Style-aware & $0.76 \pm 0.28$ & $0.80 \pm 0.22$ & $0.77 \pm 0.27$ & $0.78 \pm 0.18$ & $0.73 \pm 0.26$ & $0.77 \pm 0.24$ & $0.81 \pm 0.19$ & $0.75 \pm 0.26$ \\
\cline{2-10}
& System & $0.76 \pm 0.29$ & $0.80 \pm 0.22$ & $0.77 \pm 0.27$ & $0.77 \pm 0.18$ & $0.73 \pm 0.26$ & $0.77 \pm 0.24$ & $0.81 \pm 0.19$ & $0.74 \pm 0.27$ \\
\hline
\multirow{3}{*}{GPT-4o} & Baseline & $0.70 \pm 0.30$ & $0.75 \pm 0.26$ & $0.73 \pm 0.29$ & $0.72 \pm 0.22$ & $0.65 \pm 0.29$ & $0.72 \pm 0.28$ & $0.77 \pm 0.22$ & $0.69 \pm 0.29$ \\
\cline{2-10}
& Style-aware & $0.73 \pm 0.29$ & $0.77 \pm 0.24$ & $0.74 \pm 0.28$ & $0.75 \pm 0.20$ & $0.68 \pm 0.28$ & $0.74 \pm 0.26$ & $0.79 \pm 0.21$ & $0.71 \pm 0.28$ \\
\cline{2-10}
& System & $0.74 \pm 0.29$ & $0.78 \pm 0.23$ & $0.76 \pm 0.26$ & $0.75 \pm 0.19$ & $0.69 \pm 0.27$ & $0.75 \pm 0.25$ & $0.80 \pm 0.19$ & $0.71 \pm 0.27$ \\
\hline
\multirow{3}{*}{Qwen 2.5 Coder 32B} & Baseline & $0.74 \pm 0.29$ & $0.79 \pm 0.22$ & $0.75 \pm 0.27$ & $0.76 \pm 0.19$ & $0.70 \pm 0.28$ & $0.75 \pm 0.25$ & $0.80 \pm 0.20$ & $0.73 \pm 0.27$ \\
\cline{2-10}
& Style-aware & $0.70 \pm 0.32$ & $0.77 \pm 0.21$ & $0.72 \pm 0.31$ & $0.77 \pm 0.18$ & $0.71 \pm 0.26$ & $0.75 \pm 0.24$ & $0.80 \pm 0.19$ & $0.74 \pm 0.26$ \\
\cline{2-10}
& System & $0.73 \pm 0.29$ & $0.78 \pm 0.23$ & $0.75 \pm 0.28$ & $0.76 \pm 0.19$ & $0.70 \pm 0.27$ & $0.75 \pm 0.24$ & $0.80 \pm 0.19$ & $0.74 \pm 0.27$ \\
\hline
\end{tabular}%
}
\end{table}

The results in Table \ref{tab:comprehensive-results} reveal several key insights about model performance and prompt sensitivity:

\begin{itemize}
    \item \textbf{Model Performance Ranking}: Codestral 2501 consistently achieves the highest scores across most metrics, followed by Qwen 2.5 Coder 32B, GPT-4o, and Claude 3.5 Sonnet
    \item \textbf{Prompt Sensitivity}: The style-aware prompt (P2) shows modest improvements in some metrics, particularly for Qwen 2.5 Coder 32B in text score ($0.80 \pm 0.22$ vs baseline $0.79 \pm 0.22$)
    \item \textbf{Metric Consistency}: AST scores show the highest variability (std $\sim$ 0.28-0.37), while syntax match scores demonstrate the most consistency (std $\sim$ 0.19-0.22)
    \item \textbf{Performance Stability}: Codestral 2501 shows the most consistent performance across prompt variations, suggesting robustness to prompt engineering changes
\end{itemize}

These results provide a comprehensive foundation for understanding the effectiveness of different prompt engineering strategies and their impact on LLM-based automated program repair performance.

\subsubsection{Statistical Significance Analysis}
To rigorously evaluate the observed performance differences, we conducted comprehensive statistical testing using the Wilcoxon signed-rank test with multiple comparison corrections. Our analysis focused on identifying statistically significant differences (p < 0.01) across prompt conditions while controlling for multiple testing effects through Bonferroni correction. The statistical analysis reveals several critical insights that complement the descriptive statistics presented above.

The statistical analysis reveals several critical insights that complement the descriptive statistics presented above. For AST normalized scores, we observed significant differences between Claude 3.5 Sonnet and Codestral 2501 across all prompt conditions (p < 4.19$\times$10$^{-23}$, effect sizes 0.185-0.195), with Codestral 2501 consistently outperforming Claude 3.5 Sonnet. Similarly, Codestral 2501 significantly outperforms GPT-4o in AST normalized scores (p < 3.65$\times$10$^{-9}$, effect sizes 0.658-0.710) and CodeBLEU scores (p < 9.61$\times$10$^{-14}$, effect sizes 0.679-0.738) across all prompt conditions.

The analysis also reveals interesting model-specific patterns: Claude 3.5 Sonnet shows the weakest performance, with significantly lower scores compared to all other models in both AST normalized and CodeBLEU metrics. GPT-4o demonstrates intermediate performance, showing some competitive results against Qwen 2.5 Coder 32B in CodeBLEU (p = 1.74$\times$10$^{-4}$, effect size = 0.404) but remaining consistently below Codestral 2501. Qwen 2.5 Coder 32B shows competitive performance, particularly in CodeBLEU metrics where it achieves win rates of 26-27% against Claude 3.5 Sonnet.

These statistically significant findings, combined with the descriptive statistics, provide strong evidence that model architecture choices have substantial impacts on LLM-based program repair performance, with specialized code generation models like Codestral 2501 providing significant advantages over general-purpose language models.

\subsubsection{Model Performance Comparisons}
Beyond prompt engineering effects, our statistical analysis reveals significant performance differences between models within each prompt condition. These within-dataset comparisons (p < 0.01) provide insights into the relative capabilities of different LLM architectures for automated program repair tasks.

Across all prompt conditions, Codestral 2501 consistently demonstrates superior performance, significantly outperforming other models in both AST normalized and CodeBLEU scores. In the baseline condition, Codestral 2501 achieves significantly higher AST normalized scores compared to Claude 3.5 Sonnet (p = 4.19$\times$10$^{-23}$, effect size = 0.185), GPT-4o (p = 3.65$\times$10$^{-9}$, effect size = 0.710), and Qwen 2.5 Coder 32B (p = 1.56$\times$10$^{-5}$, effect size = 0.673). Similarly, Codestral 2501 significantly outperforms all other models in CodeBLEU scores, with effect sizes ranging from 0.678 to 0.738 across all comparisons.

The analysis reveals clear model performance hierarchies: Claude 3.5 Sonnet consistently ranks lowest across all metrics and prompt conditions, with win rates of only 15--27\% against other models. GPT-4o demonstrates intermediate performance, showing some competitive results against Qwen 2.5 Coder 32B in CodeBLEU (win rate of 30\%) but remaining consistently below Codestral 2501. Qwen 2.5 Coder 32B shows competitive performance, particularly in CodeBLEU metrics where it achieves win rates of 26--27\% against Claude 3.5 Sonnet and maintains competitive performance against GPT-4o.

These findings suggest that model architecture is the primary determinant of repair quality, with specialized code generation models like Codestral 2501 providing substantial advantages over general-purpose language models, while prompt engineering variations show more modest effects within each model's performance envelope.

\subsubsection{Statistical Results Summary}
Table \ref{tab:statistical-results} presents the comprehensive statistical analysis results, showing all significant model comparisons (p < 0.01) across different prompt conditions and metrics. The table includes p-values, effect sizes, and win/loss percentages to provide a complete picture of model performance differences.

\begin{table}[h!]
\centering
\caption{Statistical significance results for model comparisons across prompt conditions (p < 0.01).}
\label{tab:statistical-results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|c|c|c|c|c|}
\hline
\textbf{Model A} & \textbf{Model B} & \textbf{Metric} & \textbf{Prompt} & \textbf{P-value} & \textbf{Effect Size} & \textbf{Wins \%} & \textbf{Ties \%} & \textbf{Losses \%} \\
\hline
\multirow{6}{*}{Claude 3.5 Sonnet} & \multirow{2}{*}{Codestral 2501} & \multirow{2}{*}{AST Norm.} & Baseline & 4.19$\times$10$^{-23}$ & 0.185 & 16.0 & 13.3 & 70.7 \\
\cline{4-9}
& & & System & 1.73$\times$10$^{-23}$ & 0.195 & 16.7 & 14.3 & 69.0 \\
\cline{4-9}
& & & Style-aware & 6.92$\times$10$^{-22}$ & 0.185 & 15.7 & 15.3 & 69.0 \\
\cline{2-9}
& \multirow{2}{*}{Codestral 2501} & \multirow{2}{*}{CodeBLEU} & Baseline & 7.54$\times$10$^{-20}$ & 0.226 & 20.3 & 10.0 & 69.7 \\
\cline{4-9}
& & & System & 3.10$\times$10$^{-19}$ & 0.220 & 19.7 & 10.7 & 69.7 \\
\cline{4-9}
& & & Style-aware & 7.67$\times$10$^{-20}$ & 0.223 & 19.3 & 13.3 & 67.3 \\
\cline{2-9}
& \multirow{2}{*}{GPT-4o} & \multirow{2}{*}{AST Norm.} & Baseline & 1.09$\times$10$^{-10}$ & 0.328 & 29.7 & 9.7 & 60.7 \\
\cline{4-9}
& & & System & 8.59$\times$10$^{-18}$ & 0.274 & 24.3 & 11.3 & 64.3 \\
\cline{4-9}
& & & Style-aware & 1.27$\times$10$^{-13}$ & 0.271 & 23.3 & 14.0 & 62.7 \\
\cline{2-9}
& \multirow{2}{*}{GPT-4o} & \multirow{2}{*}{CodeBLEU} & Baseline & 2.33$\times$10$^{-3}$ & 0.384 & 36.0 & 6.3 & 57.7 \\
\cline{4-9}
& & & System & 5.44$\times$10$^{-7}$ & 0.358 & 32.7 & 8.7 & 58.7 \\
\cline{4-9}
& & & Style-aware & 1.44$\times$10$^{-7}$ & 0.341 & 30.0 & 12.0 & 58.0 \\
\cline{2-9}
& \multirow{2}{*}{Qwen 2.5 Coder 32B} & \multirow{2}{*}{AST Norm.} & Baseline & 1.32$\times$10$^{-14}$ & 0.261 & 22.3 & 14.3 & 63.3 \\
\cline{4-9}
& & & System & 3.41$\times$10$^{-14}$ & 0.282 & 23.7 & 16.0 & 60.3 \\
\cline{4-9}
& & & Style-aware & 8.23$\times$10$^{-10}$ & 0.310 & 25.7 & 17.3 & 57.0 \\
\cline{2-9}
& \multirow{2}{*}{Qwen 2.5 Coder 32B} & \multirow{2}{*}{CodeBLEU} & Baseline & 8.19$\times$10$^{-12}$ & 0.301 & 26.7 & 11.3 & 62.0 \\
\cline{4-9}
& & & System & 2.11$\times$10$^{-13}$ & 0.282 & 24.3 & 13.7 & 62.0 \\
\cline{4-9}
& & & Style-aware & 2.06$\times$10$^{-10}$ & 0.318 & 27.7 & 13.0 & 59.3 \\
\hline
\multirow{6}{*}{Codestral 2501} & \multirow{2}{*}{GPT-4o} & \multirow{2}{*}{AST Norm.} & Baseline & 3.65$\times$10$^{-9}$ & 0.710 & 51.3 & 27.7 & 21.0 \\
\cline{4-9}
& & & System & 9.89$\times$10$^{-5}$ & 0.658 & 43.7 & 33.7 & 22.7 \\
\cline{4-9}
& & & Style-aware & 6.95$\times$10$^{-6}$ & 0.667 & 44.7 & 33.0 & 22.3 \\
\cline{2-9}
& \multirow{2}{*}{GPT-4o} & \multirow{2}{*}{CodeBLEU} & Baseline & 9.61$\times$10$^{-14}$ & 0.738 & 55.3 & 25.0 & 19.7 \\
\cline{4-9}
& & & System & 4.89$\times$10$^{-10}$ & 0.699 & 48.7 & 30.3 & 21.0 \\
\cline{4-9}
& & & Style-aware & 8.18$\times$10$^{-8}$ & 0.679 & 47.3 & 30.3 & 22.3 \\
\cline{2-9}
& \multirow{2}{*}{Qwen 2.5 Coder 32B} & \multirow{2}{*}{AST Norm.} & Baseline & 1.56$\times$10$^{-5}$ & 0.673 & 36.3 & 46.0 & 17.7 \\
\cline{4-9}
& & & System & 1.40$\times$10$^{-5}$ & 0.675 & 37.3 & 44.7 & 18.0 \\
\cline{4-9}
& & & Style-aware & 4.58$\times$10$^{-6}$ & 0.673 & 38.3 & 43.0 & 18.7 \\
\cline{2-9}
& \multirow{2}{*}{Qwen 2.5 Coder 32B} & \multirow{2}{*}{CodeBLEU} & Baseline & 1.81$\times$10$^{-6}$ & 0.678 & 39.3 & 42.0 & 18.7 \\
\cline{4-9}
& & & System & 1.26$\times$10$^{-6}$ & 0.685 & 42.0 & 38.7 & 19.3 \\
\cline{4-9}
& & & Style-aware & 3.53$\times$10$^{-8}$ & 0.702 & 41.7 & 40.7 & 17.7 \\
\hline
\multirow{2}{*}{GPT-4o} & \multirow{2}{*}{Qwen 2.5 Coder 32B} & \multirow{2}{*}{CodeBLEU} & Baseline & 1.74$\times$10$^{-4}$ & 0.404 & 30.0 & 25.7 & 44.3 \\
\cline{4-9}
& & & System & - & - & - & - & - \\
\cline{4-9}
& & & Style-aware & - & - & - & - & - \\
\hline
\end{tabular}%
}
\end{table}

The statistical results in Table \ref{tab:statistical-results} reveal several key patterns. Codestral 2501 demonstrates overwhelming dominance across both AST normalized and CodeBLEU metrics, winning 36-55\% of comparisons against other models while losing only 17-22\%. This consistent superiority is reflected in extremely low p-values (as low as 4.19×10⁻²³) and large effect sizes (0.673-0.738), indicating that these differences are not only statistically significant but also practically meaningful.

The win/loss percentages provide additional insights into model performance: Codestral 2501's high win rates (36-55\%) against specialized models like Qwen 2.5 Coder 32B and GPT-4o suggest that its architectural advantages extend beyond simple statistical significance to consistent practical superiority. Claude 3.5 Sonnet's poor performance is evident in its low win rates (15-27\%) across all comparisons, while GPT-4o shows intermediate performance with win rates of 23-36\% depending on the comparison. Qwen 2.5 Coder 32B demonstrates competitive performance with win rates of 22-27\% against Claude 3.5 Sonnet and maintains competitive performance against GPT-4o.

