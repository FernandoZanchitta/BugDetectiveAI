{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db83bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "from data_loader.loader import load_buggy_dataset, load_data, save_data\n",
    "from bug_detective.detective import process_prompt_dataset\n",
    "from utils.visualization import compare_groundtruth_vs_corrected_histograms, compare_metrics_versus_bug_histograms, plot_column_distribution, plot_metrics_boxplots\n",
    "from utils.simple_metrics import compute_and_store_metrics\n",
    "from typing import Dict,List\n",
    "\n",
    "\n",
    "data_path = '/Users/zanchitta/Developer/BugDetectiveAI/Bugdetectiveai/data/checkpoints/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b841d5",
   "metadata": {},
   "source": [
    "### Load all Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cce6b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_identifier(df: pd.DataFrame, prompt: str) -> pd.DataFrame:\n",
    "    df['sample_uuid'] = df.apply(\n",
    "        lambda row: hashlib.md5(\n",
    "            (row['before_merge_without_docstrings'] + row['after_merge_without_docstrings']).encode('utf-8')\n",
    "        ).hexdigest(),\n",
    "        axis=1\n",
    "    )\n",
    "    df['prompt'] = prompt\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db5df028",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = load_data(data_path + \"300_metrics_prompt_baseline_system_none.pkl\")\n",
    "system_apr = load_data(data_path + \"300_metrics_prompt_baseline_system_apr.pkl\")\n",
    "prompt_style = load_data(data_path + \"300_metrics_prompt_style_based.pkl\")\n",
    "\n",
    "baseline = create_identifier(baseline,\"baseline\")\n",
    "system_apr = create_identifier(system_apr,\"system_apr\")\n",
    "prompt_style = create_identifier(prompt_style,\"prompt_style\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "230b2e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>before_merge_without_docstrings</th>\n",
       "      <th>after_merge_without_docstrings</th>\n",
       "      <th>traceback_type</th>\n",
       "      <th>full_traceback</th>\n",
       "      <th>response_qwen-2.5-coder-32b-instruct</th>\n",
       "      <th>response_codestral-2501</th>\n",
       "      <th>response_gpt-4o</th>\n",
       "      <th>response_claude-3.5-sonnet</th>\n",
       "      <th>metric_qwen-2.5-coder-32b-instruct_ast_score</th>\n",
       "      <th>metric_qwen-2.5-coder-32b-instruct_text_score</th>\n",
       "      <th>...</th>\n",
       "      <th>metric_claude-3.5-sonnet_ast_score</th>\n",
       "      <th>metric_claude-3.5-sonnet_text_score</th>\n",
       "      <th>metric_claude-3.5-sonnet_ast_score_normalized</th>\n",
       "      <th>metric_claude-3.5-sonnet_codebleu</th>\n",
       "      <th>metric_claude-3.5-sonnet_ngram_match_score</th>\n",
       "      <th>metric_claude-3.5-sonnet_weighted_ngram_match_score</th>\n",
       "      <th>metric_claude-3.5-sonnet_syntax_match_score</th>\n",
       "      <th>metric_claude-3.5-sonnet_dataflow_match_score</th>\n",
       "      <th>sample_uuid</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75748</th>\n",
       "      <td>def run(self, cmd, **kwargs):\\n        sel...</td>\n",
       "      <td>def run(self, cmd, **kwargs):\\n        sel...</td>\n",
       "      <td>IndexError</td>\n",
       "      <td>An unexpected error has occurred: IndexError: ...</td>\n",
       "      <td>def run(self, cmd, **kwargs):\\n    self._creat...</td>\n",
       "      <td>def run(self, cmd, **kwargs):\\n    self._creat...</td>\n",
       "      <td>def run(self, cmd, **kwargs):\\n    self._creat...</td>\n",
       "      <td>def run(self, cmd, **kwargs):\\n    self._creat...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498246</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.294467</td>\n",
       "      <td>0.339630</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>8caf7a732dd1acbd2731a012a8bd45fe</td>\n",
       "      <td>baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583899</th>\n",
       "      <td>def process(self, argv=None, executable=No...</td>\n",
       "      <td>def process(self, argv=None, executable=No...</td>\n",
       "      <td>TypeError</td>\n",
       "      <td>[+] Downloading '/proc/sys/kernel/core_pattern...</td>\n",
       "      <td>def process(self, argv=None, executable=None, ...</td>\n",
       "      <td>def process(self, argv=None, executable=None, ...</td>\n",
       "      <td>cwd = cwd or self.cwd\\n        if isinstance(c...</td>\n",
       "      <td>def process(self, argv=None, executable=None, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.948990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.807457</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.691194</td>\n",
       "      <td>0.705416</td>\n",
       "      <td>0.741912</td>\n",
       "      <td>0.818316</td>\n",
       "      <td>0.605140</td>\n",
       "      <td>d1e8f77aa937f586008b06aabb60d1ac</td>\n",
       "      <td>baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532193</th>\n",
       "      <td>def backprojection(calibrated_event_list, pixe...</td>\n",
       "      <td>def backprojection(calibrated_event_list, pixe...</td>\n",
       "      <td>TypeError</td>\n",
       "      <td>import sunpy.data\\n&gt;&gt;&gt; import sunpy.data.sampl...</td>\n",
       "      <td>def backprojection(calibrated_event_list, pixe...</td>\n",
       "      <td>def backprojection(calibrated_event_list, pixe...</td>\n",
       "      <td>def backprojection(calibrated_event_list, pixe...</td>\n",
       "      <td>def backprojection(calibrated_event_list, pixe...</td>\n",
       "      <td>0.835698</td>\n",
       "      <td>0.871159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714742</td>\n",
       "      <td>0.798102</td>\n",
       "      <td>0.168188</td>\n",
       "      <td>0.652772</td>\n",
       "      <td>0.529190</td>\n",
       "      <td>0.709786</td>\n",
       "      <td>0.790960</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>1ca9ee95a397f49814adbe7531b4e88a</td>\n",
       "      <td>baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357599</th>\n",
       "      <td>def export_annotations(self,export_range,e...</td>\n",
       "      <td>def export_annotations(self,export_range,e...</td>\n",
       "      <td>AttributeError</td>\n",
       "      <td>Traceback (most recent call last):\\nFile \"main...</td>\n",
       "      <td>def export_annotations(self, export_range, exp...</td>\n",
       "      <td>def export_annotations(self, export_range, exp...</td>\n",
       "      <td>def export_annotations(self, export_range, exp...</td>\n",
       "      <td>def export_annotations(self,export_range,expor...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.881880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959438</td>\n",
       "      <td>0.914406</td>\n",
       "      <td>0.919994</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2b0af7a3b93947509281e74e469190c6</td>\n",
       "      <td>baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45974</th>\n",
       "      <td>def summarize_corpus(corpus, ratio=0.2):\\n    ...</td>\n",
       "      <td>def summarize_corpus(corpus, ratio=0.2):\\n    ...</td>\n",
       "      <td>TypeError</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>def summarize_corpus(corpus, ratio=0.2):\\n    ...</td>\n",
       "      <td>def summarize_corpus(corpus, ratio=0.2):\\n    ...</td>\n",
       "      <td>def summarize_corpus(corpus, ratio=0.2):\\n    ...</td>\n",
       "      <td>def summarize_corpus(corpus, ratio=0.2):\\n    ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.892398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.765659</td>\n",
       "      <td>0.897493</td>\n",
       "      <td>0.935363</td>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>9fcbb3b0aa8818b51aba3778799ed601</td>\n",
       "      <td>baseline</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          before_merge_without_docstrings  \\\n",
       "75748       def run(self, cmd, **kwargs):\\n        sel...   \n",
       "583899      def process(self, argv=None, executable=No...   \n",
       "532193  def backprojection(calibrated_event_list, pixe...   \n",
       "357599      def export_annotations(self,export_range,e...   \n",
       "45974   def summarize_corpus(corpus, ratio=0.2):\\n    ...   \n",
       "\n",
       "                           after_merge_without_docstrings  traceback_type  \\\n",
       "75748       def run(self, cmd, **kwargs):\\n        sel...      IndexError   \n",
       "583899      def process(self, argv=None, executable=No...       TypeError   \n",
       "532193  def backprojection(calibrated_event_list, pixe...       TypeError   \n",
       "357599      def export_annotations(self,export_range,e...  AttributeError   \n",
       "45974   def summarize_corpus(corpus, ratio=0.2):\\n    ...       TypeError   \n",
       "\n",
       "                                           full_traceback  \\\n",
       "75748   An unexpected error has occurred: IndexError: ...   \n",
       "583899  [+] Downloading '/proc/sys/kernel/core_pattern...   \n",
       "532193  import sunpy.data\\n>>> import sunpy.data.sampl...   \n",
       "357599  Traceback (most recent call last):\\nFile \"main...   \n",
       "45974   ----------------------------------------------...   \n",
       "\n",
       "                     response_qwen-2.5-coder-32b-instruct  \\\n",
       "75748   def run(self, cmd, **kwargs):\\n    self._creat...   \n",
       "583899  def process(self, argv=None, executable=None, ...   \n",
       "532193  def backprojection(calibrated_event_list, pixe...   \n",
       "357599  def export_annotations(self, export_range, exp...   \n",
       "45974   def summarize_corpus(corpus, ratio=0.2):\\n    ...   \n",
       "\n",
       "                                  response_codestral-2501  \\\n",
       "75748   def run(self, cmd, **kwargs):\\n    self._creat...   \n",
       "583899  def process(self, argv=None, executable=None, ...   \n",
       "532193  def backprojection(calibrated_event_list, pixe...   \n",
       "357599  def export_annotations(self, export_range, exp...   \n",
       "45974   def summarize_corpus(corpus, ratio=0.2):\\n    ...   \n",
       "\n",
       "                                          response_gpt-4o  \\\n",
       "75748   def run(self, cmd, **kwargs):\\n    self._creat...   \n",
       "583899  cwd = cwd or self.cwd\\n        if isinstance(c...   \n",
       "532193  def backprojection(calibrated_event_list, pixe...   \n",
       "357599  def export_annotations(self, export_range, exp...   \n",
       "45974   def summarize_corpus(corpus, ratio=0.2):\\n    ...   \n",
       "\n",
       "                               response_claude-3.5-sonnet  \\\n",
       "75748   def run(self, cmd, **kwargs):\\n    self._creat...   \n",
       "583899  def process(self, argv=None, executable=None, ...   \n",
       "532193  def backprojection(calibrated_event_list, pixe...   \n",
       "357599  def export_annotations(self,export_range,expor...   \n",
       "45974   def summarize_corpus(corpus, ratio=0.2):\\n    ...   \n",
       "\n",
       "        metric_qwen-2.5-coder-32b-instruct_ast_score  \\\n",
       "75748                                       0.000000   \n",
       "583899                                      0.000000   \n",
       "532193                                      0.835698   \n",
       "357599                                      0.000000   \n",
       "45974                                       0.000000   \n",
       "\n",
       "        metric_qwen-2.5-coder-32b-instruct_text_score  ...  \\\n",
       "75748                                        0.688742  ...   \n",
       "583899                                       0.948990  ...   \n",
       "532193                                       0.871159  ...   \n",
       "357599                                       0.881880  ...   \n",
       "45974                                        0.892398  ...   \n",
       "\n",
       "        metric_claude-3.5-sonnet_ast_score  \\\n",
       "75748                             0.000000   \n",
       "583899                            0.000000   \n",
       "532193                            0.714742   \n",
       "357599                            0.000000   \n",
       "45974                             0.000000   \n",
       "\n",
       "        metric_claude-3.5-sonnet_text_score  \\\n",
       "75748                              0.498246   \n",
       "583899                             0.807457   \n",
       "532193                             0.798102   \n",
       "357599                             0.884472   \n",
       "45974                              0.909631   \n",
       "\n",
       "        metric_claude-3.5-sonnet_ast_score_normalized  \\\n",
       "75748                                        0.000000   \n",
       "583899                                       1.000000   \n",
       "532193                                       0.168188   \n",
       "357599                                       0.000000   \n",
       "45974                                        0.000000   \n",
       "\n",
       "        metric_claude-3.5-sonnet_codebleu  \\\n",
       "75748                            0.484122   \n",
       "583899                           0.691194   \n",
       "532193                           0.652772   \n",
       "357599                           0.959438   \n",
       "45974                            0.765659   \n",
       "\n",
       "        metric_claude-3.5-sonnet_ngram_match_score  \\\n",
       "75748                                     0.294467   \n",
       "583899                                    0.705416   \n",
       "532193                                    0.529190   \n",
       "357599                                    0.914406   \n",
       "45974                                     0.897493   \n",
       "\n",
       "        metric_claude-3.5-sonnet_weighted_ngram_match_score  \\\n",
       "75748                                            0.339630     \n",
       "583899                                           0.741912     \n",
       "532193                                           0.709786     \n",
       "357599                                           0.919994     \n",
       "45974                                            0.935363     \n",
       "\n",
       "        metric_claude-3.5-sonnet_syntax_match_score  \\\n",
       "75748                                      0.600000   \n",
       "583899                                     0.818316   \n",
       "532193                                     0.790960   \n",
       "357599                                     1.000000   \n",
       "45974                                      0.917647   \n",
       "\n",
       "        metric_claude-3.5-sonnet_dataflow_match_score  \\\n",
       "75748                                        0.647059   \n",
       "583899                                       0.605140   \n",
       "532193                                       0.592105   \n",
       "357599                                       1.000000   \n",
       "45974                                        0.525000   \n",
       "\n",
       "                             sample_uuid    prompt  \n",
       "75748   8caf7a732dd1acbd2731a012a8bd45fe  baseline  \n",
       "583899  d1e8f77aa937f586008b06aabb60d1ac  baseline  \n",
       "532193  1ca9ee95a397f49814adbe7531b4e88a  baseline  \n",
       "357599  2b0af7a3b93947509281e74e469190c6  baseline  \n",
       "45974   9fcbb3b0aa8818b51aba3778799ed601  baseline  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd1da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.simple_metrics import wilcoxon_test\n",
    "\n",
    "# Example: Compare two sets of metrics between experiments using Wilcoxon signed-rank test\n",
    "# Let's assume we want to compare 'accuracy' between baseline and system_apr\n",
    "def wilcoxon_test_cross_dataset_simple(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    model_name: str,\n",
    "    metric_name: str = \"ast_score\",\n",
    "    reference_column: str = \"after_merge_without_docstrings\",\n",
    "    id_column: str = \"bug_id\",\n",
    "    alternative: str = \"two-sided\"\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Perform Wilcoxon test for the same model across two datasets using existing function.\n",
    "    \n",
    "    This function reuses the existing wilcoxon_test by merging datasets and creating\n",
    "    separate response columns for each dataset.\n",
    "    \n",
    "    Args:\n",
    "        df1 (pd.DataFrame): First dataset DataFrame\n",
    "        df2 (pd.DataFrame): Second dataset DataFrame  \n",
    "        model_name (str): Name of the model to compare (e.g., \"gpt4\", \"claude\")\n",
    "        metric_name (str): Name of the metric to compare (default: \"ast_score\")\n",
    "        reference_column (str): Reference column used for metric calculation\n",
    "        id_column (str): Column name to match instances across datasets\n",
    "        alternative (str): Alternative hypothesis: \"two-sided\", \"greater\", or \"less\"\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing test results from existing wilcoxon_test\n",
    "    \"\"\"\n",
    "    # Find the response column for the specified model in each dataset\n",
    "    response_col1 = None\n",
    "    response_col2 = None\n",
    "    \n",
    "    for col in df1.columns:\n",
    "        if col.startswith(\"response_\") and model_name.lower() in col.lower():\n",
    "            response_col1 = col\n",
    "            break\n",
    "            \n",
    "    for col in df2.columns:\n",
    "        if col.startswith(\"response_\") and model_name.lower() in col.lower():\n",
    "            response_col2 = col\n",
    "            break\n",
    "    \n",
    "    if not response_col1:\n",
    "        raise ValueError(f\"Model '{model_name}' not found in dataset 1\")\n",
    "    if not response_col2:\n",
    "        raise ValueError(f\"Model '{model_name}' not found in dataset 2\")\n",
    "    \n",
    "    # Ensure metrics are computed for both datasets\n",
    "    if not has_metrics_columns(df1, response_col1):\n",
    "        print(f\"Computing metrics for dataset 1...\")\n",
    "        df1 = compute_and_store_metrics(df1, reference_column, [response_col1])\n",
    "    \n",
    "    if not has_metrics_columns(df2, response_col2):\n",
    "        print(f\"Computing metrics for dataset 2...\")\n",
    "        df2 = compute_and_store_metrics(df2, reference_column, [response_col2])\n",
    "    \n",
    "    # Get metric column names\n",
    "    clean_model_name = response_col1.replace(\"response_\", \"\")\n",
    "    metric_col1 = f\"metric_{clean_model_name}_{metric_name}\"\n",
    "    metric_col2 = f\"metric_{clean_model_name}_{metric_name}\"\n",
    "    \n",
    "    \n",
    "    merged_df = pd.merge(\n",
    "        df1[[id_column, metric_col1]], \n",
    "        df2[[id_column, metric_col2]], \n",
    "        on=id_column, \n",
    "        how='inner',\n",
    "        suffixes=('_dataset1', '_dataset2')\n",
    "    )\n",
    "    \n",
    "    # Create a unified DataFrame that mimics the structure expected by wilcoxon_test\n",
    "    unified_df = pd.DataFrame({\n",
    "        id_column: merged_df[id_column],\n",
    "        \"response_dataset1\": merged_df[metric_col1 + \"_dataset1\"],\n",
    "        \"response_dataset2\": merged_df[metric_col2 + \"_dataset2\"],\n",
    "        \"dummy_reference\": merged_df[metric_col1 + \"_dataset1\"]  # For compatibility\n",
    "    })\n",
    "    \n",
    "    # Use the existing wilcoxon_test function directly!\n",
    "    return wilcoxon_test(\n",
    "        df=unified_df,\n",
    "        response_column1=\"response_dataset1\",\n",
    "        response_column2=\"response_dataset2\",\n",
    "        metric_name=metric_name,  # This won't be used since we're passing metric values directly\n",
    "        reference_column=\"dummy_reference\",\n",
    "        alternative=alternative\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d28f6b22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metric_col1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compare GPT-4 performance across experiments using existing function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mwilcoxon_test_cross_dataset_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_apr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqwen-2.5-coder-32b-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcodebleu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_uuid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross-dataset Wilcoxon test results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp-value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 21\u001b[0m, in \u001b[0;36mwilcoxon_test_cross_dataset_simple\u001b[0;34m(df1, df2, model_name, metric_name, reference_column, id_column, alternative)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Simplified cross-dataset Wilcoxon test using existing function.\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Find response columns and compute metrics (same as above)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# ... (same code as above) ...\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Merge datasets\u001b[39;00m\n\u001b[1;32m     20\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[0;32m---> 21\u001b[0m     df1[[id_column, \u001b[43mmetric_col1\u001b[49m]], \n\u001b[1;32m     22\u001b[0m     df2[[id_column, metric_col2]], \n\u001b[1;32m     23\u001b[0m     on\u001b[38;5;241m=\u001b[39mid_column, \n\u001b[1;32m     24\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     25\u001b[0m     suffixes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dataset1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dataset2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Create a unified DataFrame that mimics the structure expected by wilcoxon_test\u001b[39;00m\n\u001b[1;32m     29\u001b[0m unified_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     30\u001b[0m     id_column: merged_df[id_column],\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_dataset1\u001b[39m\u001b[38;5;124m\"\u001b[39m: merged_df[metric_col1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_dataset1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_dataset2\u001b[39m\u001b[38;5;124m\"\u001b[39m: merged_df[metric_col2 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_dataset2\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdummy_reference\u001b[39m\u001b[38;5;124m\"\u001b[39m: merged_df[metric_col1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_dataset1\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# For compatibility\u001b[39;00m\n\u001b[1;32m     34\u001b[0m })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metric_col1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare GPT-4 performance across experiments using existing function\n",
    "results = wilcoxon_test_cross_dataset_simple(\n",
    "    df1=baseline,\n",
    "    df2=system_apr,\n",
    "    model_name=\"qwen-2.5-coder-32b-instruct\",\n",
    "    metric_name=\"codebleu\",\n",
    "    id_column=\"sample_uuid\"\n",
    ")\n",
    "\n",
    "print(f\"Cross-dataset Wilcoxon test results:\")\n",
    "print(f\"p-value: {results['pvalue']:.6f}\")\n",
    "print(f\"Significant difference: {'Yes' if results['significant'] else 'No'}\")\n",
    "print(f\"Effect size: {results['effect_size']:.4f}\")\n",
    "print(f\"Matching instances: {results['matching_instances']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BugDetectiveAI (Poetry)",
   "language": "python",
   "name": "bugdetectiveai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
